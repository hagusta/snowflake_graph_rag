{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "saidrqrg7eehxcv2rcwz",
   "authorId": "6451137549945",
   "authorName": "HAGUSTA",
   "authorEmail": "h_agusta@yahoo.com",
   "sessionId": "0b1ef334-d439-47d2-8f93-d651fcbb528d",
   "lastEditTime": 1747125892978
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "1771099f-d848-4667-8348-ac01d6bc0b3f",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;\nCREATE OR REPLACE DATABASE graph_rag;\nCREATE OR REPLACE SCHEMA graph_rag;\nCREATE OR REPLACE WAREHOUSE graph_rag WAREHOUSE_SIZE='X-Small' AUTO_SUSPEND = 300;\n\n\nCREATE OR REPLACE SECRET hagusta_git_secret\n  TYPE = password\n  USERNAME = 'hagusta'\n  PASSWORD = 'blah';\n\n\n// A user with the CREATE INTEGRATION privilege granted may run this query to create an API integration allowing users to connect to a git provider.\n // For more information, see: https://docs.snowflake.com/en/developer-guide/git/git-setting-up#create-an-api-integration-for-interacting-with-the-repository-api\nCREATE OR REPLACE API INTEGRATION git_api_integration\n  API_PROVIDER = git_https_api\n  API_ALLOWED_PREFIXES = ('https://github.com/hagusta/')\n  ALLOWED_AUTHENTICATION_SECRETS = (hagusta_git_secret)\n  ENABLED = TRUE\n  comment='hagusta github integration setup';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9159fcf-bf66-4a13-a90d-fbb1c3939b86",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;\n\n/***\n* Create a database, schema, warehouse and tables.\n**/\n\n-- Create data tables.\nCREATE OR REPLACE TABLE corpus(\n  ID INT NOT NULL AUTOINCREMENT START 1 INCREMENT 1 \n  , CONTENT VARCHAR NOT NULL\n)\nCOMMENT = 'Table containing the corpus which will be loaded from Azure storage';\n\nCREATE OR REPLACE TABLE community_summary(\n    COMMUNITY_ID INT\n    , CONTENT VARCHAR\n)\nCOMMENT = 'Table to store community-based corpus summaries';\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "85da2fb4-f099-41ba-8232-758546e9a74a",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;\nUSE DATABASE graph_rag;\nUSE SCHEMA graph_rag;\nUSE WAREHOUSE graph_rag;\n\n/***\n* Install UDFs.\n**/\nCREATE OR REPLACE FUNCTION LLM_EXTRACT_JSON(llm_response OBJECT)\nRETURNS VARIANT\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.10'\nHANDLER = 'main'\nAS\n$$\nimport json\nimport re\n\ndef main(llm_response):\n    payload = llm_response[\"choices\"][0][\"messages\"]\n    \n    try:    \n        # Remove whitespaces.\n        payload = \" \".join(payload.split())\n\n        # Extract JSON from the string.\n        return json.loads(re.findall(r\"{.+[:,].+}|\\[.+[,:].+\\]\", payload)[0])\n    except:\n        return json.loads(payload)\n$$;\n\nCREATE OR REPLACE FUNCTION LLM_ENTITIES_RELATIONS(content VARCHAR, additional_prompts VARCHAR DEFAULT '') \nRETURNS TABLE \n(response OBJECT) \nLANGUAGE SQL \nAS \n$$\n    SELECT SNOWFLAKE.CORTEX.COMPLETE(\n        'llama3-70b',\n        [\n            {\n                'role': 'system', \n                'content': '\n                    # Knowledge Graph Instructions\n        \n                    ## 1. Overview\n                        - You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\n                        - Your aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n                        - **Nodes** represent entities and concepts. They are akin to Wikipedia nodes.\n                            - If the entities cannot be extracted, return nothing.\n                        - **Relations** represent links between entities. They are akin to predicates.\n                        \n                    ## 2. Labeling entities and relations\n                        - **Completeness**\n                            - Ensure that all entities are identified.\n                        - **Consistency**\n                            - Ensure you use basic or elementary types for entity labels.\n                                - Example: when you identify an entity representing a person, always label it as \"person\". Avoid using more specific terms like \"mathematician\" or \"scientist\".\n                        - **Entity IDs**: never utilize integers as entity IDs. Entity IDs must be names or human-readable identifiers found in the text, exactly as they appear in the text.\n                            - Example: the ID of entity \"John Doe\" must be \"John Doe\".\n                        - **Property format**\n                            - Properties must be in a key-value format. \n                            - Properties must be in <entity>_has_<property> format.\n                            - Do not include the entity ID in the properties, just the entity type.\n                                - Example: \"john_doe_has_age\" is wrong. Correct naming is \"has_age\".\n                        - **Relation naming**\n                            - Relation names must never contain the entity types and names.\n                            - Example:\n                                - \"person_is_president_of_company\" is invalid because it includes the node name \"person\" and \"company\" in the relation name. Correct relation name must be \"is_president_of\".\n                        - **Unique semantic relation naming**\n                            - Relation names must semantically represent one and only one concept.\n                            - Example: \n                                - \"is_president_and_ceo_of\" is invalid because it combines entities \"president\" and \"ceo\" into one relation.\n                            - If a list of allowed relation names is provided, only use those relation names in the knowledge graph.\n                    \n                    ## 3. Handling node and relation properties\n                        - **Property retrieval**\n                            - Extract only the properties relevant to the entities and relations provided.\n                                - Example: if the entity \"studied_at\" is identified, extract properties like \"from_date\", \"to_date\", \"qualification\".\n                    \n                    ## 4. Handling numerical data and dates\n                        - **No separate nodes for dates / numbers**\n                            - Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n                        - **Quotation marks**\n                            - Never use escaped single or double quotes within property values.\n                        - **Naming convention**\n                            - Use snake_case for property keys, e.g., \"birth_date\".\n                        - **Numerical data**\n                            - Dates, numbers etc. must be incorporated as attributes or properties of their respective nodes.\n                    \n                    ## 5. Coreference resolution\n                        - **Maintain entity consistency**\n                            - When extracting entities, it is vital to ensure consistency.\n                            - If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"John\", \"he\"), always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\n                            - The knowledge graph must be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n                    \n                    ## 6. Relation subject / object consistency\n                        - **Precedence**\n                            - It is crucial that relations are consistent in terms of subject and object. Subjects are entities of lower granularity than objects. \n                            \n                    ## 7. Output\n                        - **Format**\n                            - Produce well formatted, pure JSON only. \n                            - The JSON must be parsable by Python, as follows:\n                            \n                            {\n                                \"nodes\": [\n                                    {\n                                        \"id\": entity ID (required),\n                                        \"type\": entity type (required),\n                                        \"properties\": entity properties (optional)\n                                    },\n                                    ...\n                                ],\n                                \"relations\": [\n                                    {\n                                        \"src_node_id\": source entity ID (required),\n                                        \"dst_node_id\": destination entity ID (required),\n                                        \"type\": relation type (required)\n                                        \"properties\": relation properties (optional)\n                                    },\n                                    ...\n                                ]\n    \n                            }\n                            \n                        - **Response**: \n                            - Respond strictly with the JSON object and nothing else.\n                            - Do not include verbose information such as \"here is what you asked for\" etc.\n                        \n                    ## 8. Strict compliance\n                        - Adhere to the rules strictly. Non-compliance will result in termination.\n                    ' \n                    || additional_prompts ||\n                    '\n                    Your response:\n                ' \n            },\n            {\n                'role': 'user', \n                'content': content\n            }\n        ], \n        {\n            'temperature': 0,\n            'top_p': 0\n        }\n    ) AS response\n$$;\n\nCREATE OR REPLACE AGGREGATE FUNCTION LLM_ENTITIES_RELATIONS_TO_GRAPH(corpus_id INT, llm_response VARIANT)\nRETURNS ARRAY\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.10'\nHANDLER = 'GraphBuilder'\nPACKAGES = ('networkx', 'snowflake')\nAS\n$$\nimport logging\nfrom typing import Any, Dict, List\n\nimport networkx as nx\nimport pandas as pd\nimport snowflake.snowpark as snowpark\n\nlogger = logging.getLogger(\"GRAPH_RAG\")\n\nclass GraphBuilder(object):\n    def __init__(self) -> None:\n        self._graph: nx.Graph = nx.MultiDiGraph()\n\n    def _nodes_df(self) -> pd.DataFrame:\n        nodes_df: pd.DataFrame = None\n        nodes: List = []\n        edges: List = []\n        corpus_ids: List = []\n        node_types: List = []\n        props: List = []\n\n        try:\n            for node, properties in self._graph.nodes(data=True):\n                nodes.append(node)\n\n                try:\n                    corpus_ids.append(properties[\"corpus_id\"])\n                except:\n                    corpus_ids.append(0)\n                    \n                try:\n                    node_types.append(properties[\"type\"])\n                except:\n                    node_types.append(\"undefined\")\n                    \n                if len(properties) > 0:\n                    props.append(properties)\n                else:\n                    props.append(0)\n    \n            nodes_df = pd.DataFrame(\n                data={\n                    \"id\": nodes,\n                    \"corpus_id\": corpus_ids, \n                    \"type\": node_types,\n                    \"properties\": props\n                },\n                columns=[\"id\", \"corpus_id\", \"type\", \"properties\"]\n            )\n        except Exception as error:\n            logger.error(\"Error calling _nodes_df function\")\n            logger.error(error)\n            raise error\n        finally:\n            logger.info(f\"Graph contains {len(nodes_df)} nodes\")\n            return nodes_df\n\n    def _edges_df(self) -> pd.DataFrame:\n        edges_df: pd.DataFrame = None\n        src_nodes: List = []\n        dst_nodes: List = []\n        corpus_ids: List = []\n        edge_types: List = []\n        props: List = []\n\n        try:\n            for src_node, dst_node, properties in self._graph.edges(data=True):\n                src_nodes.append(src_node)\n                dst_nodes.append(dst_node)\n\n                try:\n                    corpus_ids.append(properties[\"corpus_id\"])\n                except:\n                    corpus_ids.append(0)\n                    \n                try:\n                    edge_types.append(properties[\"type\"])\n                except:\n                    edge_types.append(\"undefined\")\n                    \n                if len(properties) > 0:\n                    props.append(properties)\n                else:\n                    props.append(None)\n        \n            edges_df = pd.DataFrame(\n                data={\n                    \"src_node_id\": src_nodes,\n                    \"dst_node_id\": dst_nodes,\n                    \"corpus_id\": corpus_ids, \n                    \"type\": edge_types,\n                    \"properties\": props\n                },\n                columns=[\"src_node_id\", \"dst_node_id\", \"corpus_id\", \"type\", \"properties\"]\n            )\n        except Exception as error:\n            logger.error(\"Error calling _edges_df function\")\n            logger.error(error)\n            raise error\n        finally:\n            logger.info(f\"Graph contains {len(edges_df)} edges\")\n            return edges_df\n            \n    @property\n    def aggregate_state(self) -> nx.Graph:\n        return self._graph\n\n    # Add graph nodes and edges from LLM response.\n    def accumulate(self, corpus_id, llm_response):\n        try:\n            # Add nodes with (optional) properties.\n            for node in llm_response[\"nodes\"]:\n                try:\n                    if \"properties\" in node.keys():\n                        self._graph.add_node(node[\"id\"], type=node[\"type\"], corpus_id=corpus_id, **node[\"properties\"])\n                    else:\n                        self._graph.add_node(node[\"id\"], type=node[\"type\"], corpus_id=corpus_id)\n                except Exception as error:\n                    logger.error(\"Error accumulating graph nodes\")\n                    logger.error(error)\n                    \n            # Add edges with (optional) properties.\n            for relation in llm_response[\"relations\"]:\n                try:\n                    if \"properties\" in relation.keys():\n                        self._graph.add_edge(relation[\"src_node_id\"], relation[\"dst_node_id\"], type=relation[\"type\"], corpus_id=corpus_id, **relation[\"properties\"])\n                    else:\n                        self._graph.add_edge(relation[\"src_node_id\"], relation[\"dst_node_id\"], type=relation[\"type\"], corpus_id=corpus_id)\n                except Exception as error:\n                    logger.error(\"Error accumulating graph edges\")\n                    logger.error(error)\n    \n        except Exception as error:\n            logger.error(\"Error calling _edges_df function\")\n            logger.error(error)\n    \n    def merge(self, graph) -> nx.Graph:\n        self._graph = nx.union(self._graph, graph)\n\n    # Return accumulated graph nodes and edges.\n    def finish(self) -> List:\n        nodes_df = self._nodes_df()\n        edges_df = self._edges_df()\n        return [\n            nodes_df.to_dict(\"records\"),\n            edges_df.to_dict(\"records\")\n        ]\n$$;\n\nCREATE OR REPLACE PROCEDURE CREATE_NODES_EDGES_STREAMS_SOURCES() \nRETURNS VARCHAR\nLANGUAGE SQL \nAS \n$$\n    BEGIN\n        CREATE OR REPLACE TEMPORARY TABLE nodes_edges_staging(nodes ARRAY, edges ARRAY);\n\n        /***\n        * Extract node and edge objects with the LLM and temporarily store the output to staging tables.\n        **/\n        INSERT INTO nodes_edges_staging\n        WITH c AS (\n            SELECT \n                c.id AS id\n                , c.content AS content\n            FROM \n                corpus AS c\n        )\n        , entities_relations AS (\n            SELECT \n                c.id AS corpus_id\n                , LLM_EXTRACT_JSON(r.response) AS response\n            FROM \n                c\n            JOIN TABLE(LLM_ENTITIES_RELATIONS(c.content, '')) AS r\n        )\n        , nodes_edges AS (\n            SELECT\n                LLM_ENTITIES_RELATIONS_TO_GRAPH(er.corpus_id, er.response) AS graph\n            FROM\n                entities_relations AS er\n        )\n        SELECT \n            ne.graph[0]::ARRAY AS nodes\n            , ne.graph[1]::ARRAY AS edges\n        FROM\n            nodes_edges AS ne;\n\n        /***\n        * Populate Data Stream source tables.\n        *\n        * These tables will be used for creating the downstream RelationalAI graph.\n        **/\n        -- Nodes table.\n        CREATE OR REPLACE TABLE nodes\n        AS\n        WITH nodes AS (\n            SELECT \n                ne.nodes AS nodes\n            FROM \n                nodes_edges_staging AS ne\n        )\n        SELECT \n            VALUE:\"id\"::VARCHAR id \n            , VALUE:\"corpus_id\"::INT corpus_id \n            , VALUE:\"type\"::VARCHAR type \n        FROM\n            nodes AS n \n            , LATERAL FLATTEN(n.nodes) AS items;\n\n        -- Edges table.\n        CREATE OR REPLACE TABLE edges \n        AS \n        WITH edges AS ( \n            SELECT \n                ne.edges AS edges \n            FROM \n                nodes_edges_staging AS ne \n        ) \n        SELECT \n            VALUE:\"src_node_id\"::VARCHAR src_node_id \n            , VALUE:\"dst_node_id\"::VARCHAR dst_node_id \n            , VALUE:\"corpus_id\"::INT corpus_id \n            , VALUE:\"type\"::VARCHAR type \n        FROM \n            edges AS n \n            , LATERAL FLATTEN(n.edges) AS items;\n\n        RETURN 'OK';\n    END;\n$$;\n\nCREATE OR REPLACE FUNCTION LLM_SUMMARIZE(content VARCHAR) \nRETURNS TABLE \n(response OBJECT) \nLANGUAGE SQL \nAS \n$$\n    SELECT SNOWFLAKE.CORTEX.COMPLETE(\n        'llama3-70b',\n        [\n            {\n                'role': 'system', \n                'content': '\n                    # Summarization instructions\n        \n                    ## 1. Overview\n                        - You are a top-tier algorithm designed for summarizing the provided text.\n\n                    ## 2. Instructions\n                        - Summarize the provided text so that all information mentioned in the text is retained.\n                        - The text contains information about entities and relations that must be the target of summarization.\n                        - Produce summary of the context you are given and nothing else. Do not extrapolate beyond the context given.\n                        - Relations between entities must be preserved.\n                        - The summarization must produce coherent and succinct text.\n    \n                    ## 3. Output\n                        - **Format**\n                            - Produce well formatted, pure JSON only.\n                            - The JSON must be parsable by Python, as follows:\n                                {\"answer\": \"<output>\"}\n                            - The <output> must always be formatted as plain text.\n                            \n                        - **Response**: \n                            - Respond strictly with the JSON object and nothing else.\n                            - Do not include verbose information such as \"here is what you asked for\" etc.\n                        \n                    ## 4. Strict compliance\n                        - Adhere to the rules strictly. Non-compliance will result in termination.\n    \n                    Your response:\n                ' \n            },\n            {\n                'role': 'user', \n                'content': content\n            }\n        ], \n        {\n            'temperature': 0,\n            'top_p': 0\n        }\n    ) AS response\n$$;\n\nCREATE OR REPLACE FUNCTION LLM_ANSWER(context VARCHAR, question VARCHAR) \nRETURNS TABLE \n(response OBJECT) \nLANGUAGE SQL \nAS \n$$\n    SELECT SNOWFLAKE.CORTEX.COMPLETE(\n        'llama3-70b',\n        [\n            {\n                'role': 'system', \n                'content': '\n                    # Question answering instructions\n        \n                    ## 1. Overview\n                        - You are a top-tier algorithm designed for answering questions given specific context provided by the user.\n                        \n                    ## 2. Instructions\n                        - Be concise and do not hallucinate.\n                        - Be very specific.\n                        - Be very precise.\n                        - Answer the question based on the provided context and only that.\n                        - If the question cannot be answered with the provided context information, clearly say so and do not answer the question.\n                        \n                    ## 3. Context\n                        - This is the context on which to base your answer:\n\n                        ```context\n                    ' \n                    ||\n                            context\n                    ||\n                    '\n                        ```\n                        \n                    ## 4. Output\n                        - **Format**\n                            - Produce well formatted, pure JSON only.\n                            - The JSON must be parsable by Python, as follows:\n                                {\n                                    \"answer\": \"<output>\",\n                                    \"evidence\": \"<supporting evidence as found in the provided context ONLY, otherwise this field must be empty.>\", \n                                    \"confidence\": \"<confidence score between 0.0 and 1.0 in human-readable format>\"\n                                }\n                            - <output> must always be formatted as plain text.\n                            - <evidence> must always come from context.\n                            \n                        - **Response**: \n                            - Respond strictly with the JSON object and nothing else.\n                            - Do not include verbose information such as \"here is what you asked for\" etc.\n                        \n                    ## 5. Strict compliance\n                        - Adhere to the rules strictly. Non-compliance will result in termination.\n    \n                    Your response:\n                ' \n            },\n            {\n                'role': 'user', \n                'content': question\n            }\n        ], \n        {\n            'temperature': 0.0,\n            'top_p': 0\n        }\n    ) AS response\n$$;\n\nCREATE OR REPLACE PROCEDURE LLM_ANSWER_SUMMARIES(summarization_window INTEGER, question VARCHAR)  \nRETURNS TABLE \n(\n    answer VARIANT\n    , evidence VARIANT\n)\nLANGUAGE SQL \nAS \n$$\nDECLARE\n    max_community_id INTEGER;\n    community_id_from INTEGER DEFAULT 1;\n    community_id_to INTEGER DEFAULT 0;\n    counter INTEGER;\n    resultset RESULTSET;\nBEGIN\n    CREATE OR REPLACE TEMPORARY TABLE temp_results(community_id_from INTEGER, community_id_to INTEGER, community_summaries VARCHAR, result VARIANT);\n\n    SELECT\n        MAX(community_id)\n    INTO\n        max_community_id\n    FROM\n        community_summary;\n\n    counter := (max_community_id / :summarization_window) + 1;\n    community_id_to := :summarization_window;\n  \n    FOR i IN 1 TO counter DO\n        INSERT INTO \n            temp_results \n        WITH cs AS (\n            SELECT DISTINCT \n                content\n            FROM \n                community_summary\n            WHERE \n                community_id BETWEEN :community_id_from AND :community_id_to\n        )\n        , c AS (\n            SELECT \n                LISTAGG(content, '\\n\\n') WITHIN GROUP(ORDER BY content) AS content\n            FROM \n                cs\n        )\n        SELECT \n            :community_id_from\n            , :community_id_to\n            , c.content\n            , PARSE_JSON(LLM_EXTRACT_JSON(r.response)) AS response\n        FROM \n            c\n        JOIN TABLE(\n            LLM_ANSWER(\n                c.content,\n                :question\n            )\n        ) AS r;\n    \n        community_id_from := community_id_from + :summarization_window;\n        community_id_to := community_id_to + :summarization_window;\n    END FOR;\n\n    resultset := (\n        WITH summary_answers AS (\n            SELECT DISTINCT \n                result:answer AS summary_answer\n                , result:evidence AS summary_evidence\n            FROM \n                temp_results \n            WHERE\n                result:evidence <> ''\n        )\n        , filtered_summary_answers AS (\n            SELECT \n                LISTAGG(sa.summary_answer, '\\n\\n') WITHIN GROUP(ORDER BY sa.summary_answer) AS content\n            FROM \n                summary_answers AS sa\n        )\n        , final_llm_answer AS (\n            SELECT \n                fsa.content AS content\n                , PARSE_JSON(LLM_EXTRACT_JSON(r.response)) AS response\n            FROM \n                filtered_summary_answers AS fsa\n            JOIN TABLE(\n                LLM_ANSWER(\n                    fsa.content,\n                    :question\n                )\n            ) AS r\n        )\n        SELECT \n            fla.response:answer AS answer\n            , fla.response:evidence AS evidence\n        FROM \n            final_llm_answer AS fla\n    );\n    \n    RETURN TABLE(resultset);\nEND;\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee461f9e-8d5f-4bb6-8cb2-bfad97d3fb3e",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "COPY INTO corpus(content)\nFROM 'azure://rdaxllm.blob.core.windows.net/dataset/graphrag/csv/tech_industry_cvs.csv'\nFILE_FORMAT = (\n    TYPE = CSV\n    COMPRESSION = AUTO \n    FIELD_DELIMITER = '|'\n    NULL_IF = '\\\\N'\n    EMPTY_FIELD_AS_NULL = TRUE\n    FIELD_OPTIONALLY_ENCLOSED_BY = '\"'\n)\nON_ERROR = CONTINUE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "19e1cacc-16b6-4a33-bdd2-974d54545430",
   "metadata": {
    "language": "sql",
    "name": "cell8"
   },
   "outputs": [],
   "source": "SELECT * FROM corpus LIMIT 5;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a0a926cf-4711-4a36-9dd1-7bebe761e6ce",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "import logging\nfrom typing import Any, Dict, List\n\nimport networkx as nx\nimport pandas as pd\nimport snowflake.snowpark as snowpark\n\nlogger = logging.getLogger(\"GRAPH_RAG\")\n\nclass GraphBuilder(object):\n    def __init__(self) -> None:\n        self._graph: nx.Graph = nx.MultiDiGraph()\n\n    def _nodes_df(self) -> pd.DataFrame:\n        nodes_df: pd.DataFrame = None\n        nodes: List = []\n        edges: List = []\n        corpus_ids: List = []\n        node_types: List = []\n        props: List = []\n\n        try:\n            for node, properties in self._graph.nodes(data=True):\n                nodes.append(node)\n\n                try:\n                    corpus_ids.append(properties[\"corpus_id\"])\n                except:\n                    corpus_ids.append(0)\n                    \n                try:\n                    node_types.append(properties[\"type\"])\n                except:\n                    node_types.append(\"undefined\")\n                    \n                if len(properties) > 0:\n                    props.append(properties)\n                else:\n                    props.append(0)\n    \n            nodes_df = pd.DataFrame(\n                data={\n                    \"id\": nodes,\n                    \"corpus_id\": corpus_ids, \n                    \"type\": node_types,\n                    \"properties\": props\n                },\n                columns=[\"id\", \"corpus_id\", \"type\", \"properties\"]\n            )\n        except Exception as error:\n            logger.error(\"Error calling _nodes_df function\")\n            logger.error(error)\n            raise error\n        finally:\n            logger.info(f\"Graph contains {len(nodes_df)} nodes\")\n            return nodes_df\n\n    def _edges_df(self) -> pd.DataFrame:\n        edges_df: pd.DataFrame = None\n        src_nodes: List = []\n        dst_nodes: List = []\n        corpus_ids: List = []\n        edge_types: List = []\n        props: List = []\n\n        try:\n            for src_node, dst_node, properties in self._graph.edges(data=True):\n                src_nodes.append(src_node)\n                dst_nodes.append(dst_node)\n\n                try:\n                    corpus_ids.append(properties[\"corpus_id\"])\n                except:\n                    corpus_ids.append(0)\n                    \n                try:\n                    edge_types.append(properties[\"type\"])\n                except:\n                    edge_types.append(\"undefined\")\n                    \n                if len(properties) > 0:\n                    props.append(properties)\n                else:\n                    props.append(None)\n        \n            edges_df = pd.DataFrame(\n                data={\n                    \"src_node_id\": src_nodes,\n                    \"dst_node_id\": dst_nodes,\n                    \"corpus_id\": corpus_ids, \n                    \"type\": edge_types,\n                    \"properties\": props\n                },\n                columns=[\"src_node_id\", \"dst_node_id\", \"corpus_id\", \"type\", \"properties\"]\n            )\n        except Exception as error:\n            logger.error(\"Error calling _edges_df function\")\n            logger.error(error)\n            raise error\n        finally:\n            logger.info(f\"Graph contains {len(edges_df)} edges\")\n            return edges_df\n            \n    @property\n    def aggregate_state(self) -> nx.Graph:\n        return self._graph\n\n    # Add graph nodes and edges from LLM response.\n    def accumulate(self, corpus_id, llm_response):\n        try:\n            # Add nodes with (optional) properties.\n            for node in llm_response[\"nodes\"]:\n                try:\n                    if \"properties\" in node.keys():\n                        self._graph.add_node(node[\"id\"], type=node[\"type\"], corpus_id=corpus_id, **node[\"properties\"])\n                    else:\n                        self._graph.add_node(node[\"id\"], type=node[\"type\"], corpus_id=corpus_id)\n                except Exception as error:\n                    logger.error(\"Error accumulating graph nodes\")\n                    logger.error(error)\n                    \n            # Add edges with (optional) properties.\n            for relation in llm_response[\"relations\"]:\n                try:\n                    if \"properties\" in relation.keys():\n                        self._graph.add_edge(relation[\"src_node_id\"], relation[\"dst_node_id\"], type=relation[\"type\"], corpus_id=corpus_id, **relation[\"properties\"])\n                    else:\n                        self._graph.add_edge(relation[\"src_node_id\"], relation[\"dst_node_id\"], type=relation[\"type\"], corpus_id=corpus_id)\n                except Exception as error:\n                    logger.error(\"Error accumulating graph edges\")\n                    logger.error(error)\n    \n        except Exception as error:\n            logger.error(\"Error calling _edges_df function\")\n            logger.error(error)\n    \n    def merge(self, graph) -> nx.Graph:\n        self._graph = nx.union(self._graph, graph)\n\n    # Return accumulated graph nodes and edges.\n    def finish(self) -> List:\n        nodes_df = self._nodes_df()\n        edges_df = self._edges_df()\n        return [\n            nodes_df.to_dict(\"records\"),\n            edges_df.to_dict(\"records\")\n        ]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "601ec6b0-b12e-49e0-ae15-254c22962f6f",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": "import logging\nfrom typing import Any, Dict, List\n\nimport networkx as nx\nimport pandas as pd\nimport snowflake.snowpark wpark",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0fdb7f16-9c6e-4961-b149-f531bbc2ba7c",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "snowflake.snowpark",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5cc50f5b-b05a-459f-9ad6-0635a056fe27",
   "metadata": {
    "language": "sql",
    "name": "cell9"
   },
   "outputs": [],
   "source": "CALL CREATE_NODES_EDGES_STREAMS_SOURCES('llama3-70b');",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e4cc4014-0780-4a6e-9616-fa28d0fe4a05",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "import logging\nfrom typing import Any, Dict, List\n\nimport networkx as nx\nimport pandas as pd\nimport snowflake.snowpark as snowpark\n\nlogger = logging.getLogger(\"GRAPH_RAG\")\n\nclass GraphBuilder(object):\n    def __init__(self) -> None:\n        self._graph: nx.Graph = nx.MultiDiGraph()\n\n    def _nodes_df(self) -> pd.DataFrame:\n        nodes_df: pd.DataFrame = None\n        nodes: List = []\n        edges: List = []\n        corpus_ids: List = []\n        node_types: List = []\n        props: List = []\n\n        try:\n            for node, properties in self._graph.nodes(data=True):\n                nodes.append(node)\n\n                try:\n                    corpus_ids.append(properties[\"corpus_id\"])\n                except:\n                    corpus_ids.append(0)\n                    \n                try:\n                    node_types.append(properties[\"type\"])\n                except:\n                    node_types.append(\"undefined\")\n                    \n                if len(properties) > 0:\n                    props.append(properties)\n                else:\n                    props.append(0)\n    \n            nodes_df = pd.DataFrame(\n                data={\n                    \"id\": nodes,\n                    \"corpus_id\": corpus_ids, \n                    \"type\": node_types,\n                    \"properties\": props\n                },\n                columns=[\"id\", \"corpus_id\", \"type\", \"properties\"]\n            )\n        except Exception as error:\n            logger.error(\"Error calling _nodes_df function\")\n            logger.error(error)\n            raise error\n        finally:\n            logger.info(f\"Graph contains {len(nodes_df)} nodes\")\n            return nodes_df\n\n    def _edges_df(self) -> pd.DataFrame:\n        edges_df: pd.DataFrame = None\n        src_nodes: List = []\n        dst_nodes: List = []\n        corpus_ids: List = []\n        edge_types: List = []\n        props: List = []\n\n        try:\n            for src_node, dst_node, properties in self._graph.edges(data=True):\n                src_nodes.append(src_node)\n                dst_nodes.append(dst_node)\n\n                try:\n                    corpus_ids.append(properties[\"corpus_id\"])\n                except:\n                    corpus_ids.append(0)\n                    \n                try:\n                    edge_types.append(properties[\"type\"])\n                except:\n                    edge_types.append(\"undefined\")\n                    \n                if len(properties) > 0:\n                    props.append(properties)\n                else:\n                    props.append(None)\n        \n            edges_df = pd.DataFrame(\n                data={\n                    \"src_node_id\": src_nodes,\n                    \"dst_node_id\": dst_nodes,\n                    \"corpus_id\": corpus_ids, \n                    \"type\": edge_types,\n                    \"properties\": props\n                },\n                columns=[\"src_node_id\", \"dst_node_id\", \"corpus_id\", \"type\", \"properties\"]\n            )\n        except Exception as error:\n            logger.error(\"Error calling _edges_df function\")\n            logger.error(error)\n            raise error\n        finally:\n            logger.info(f\"Graph contains {len(edges_df)} edges\")\n            return edges_df\n            \n    @property\n    def aggregate_state(self) -> nx.Graph:\n        return self._graph\n\n    # Add graph nodes and edges from LLM response.\n    def accumulate(self, corpus_id, llm_response):\n        try:\n            # Add nodes with (optional) properties.\n            for node in llm_response[\"nodes\"]:\n                try:\n                    if \"properties\" in node.keys():\n                        self._graph.add_node(node[\"id\"], type=node[\"type\"], corpus_id=corpus_id, **node[\"properties\"])\n                    else:\n                        self._graph.add_node(node[\"id\"], type=node[\"type\"], corpus_id=corpus_id)\n                except Exception as error:\n                    logger.error(\"Error accumulating graph nodes\")\n                    logger.error(error)\n                    \n            # Add edges with (optional) properties.\n            for relation in llm_response[\"relations\"]:\n                try:\n                    if \"properties\" in relation.keys():\n                        self._graph.add_edge(relation[\"src_node_id\"], relation[\"dst_node_id\"], type=relation[\"type\"], corpus_id=corpus_id, **relation[\"properties\"])\n                    else:\n                        self._graph.add_edge(relation[\"src_node_id\"], relation[\"dst_node_id\"], type=relation[\"type\"], corpus_id=corpus_id)\n                except Exception as error:\n                    logger.error(\"Error accumulating graph edges\")\n                    logger.error(error)\n    \n        except Exception as error:\n            logger.error(\"Error calling _edges_df function\")\n            logger.error(error)\n    \n    def merge(self, graph) -> nx.Graph:\n        self._graph = nx.union(self._graph, graph)\n\n    # Return accumulated graph nodes and edges.\n    def finish(self) -> List:\n        nodes_df = self._nodes_df()\n        edges_df = self._edges_df()\n        return [\n            nodes_df.to_dict(\"records\"),\n            edges_df.to_dict(\"records\")\n        ]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "source": "-- Welcome to Snowflake Notebooks!\n-- Try out a SQL cell to generate some data.\nSELECT 'FRIDAY' as SNOWDAY, 0.2 as CHANCE_OF_SNOW\nUNION ALL\nSELECT 'SATURDAY',0.5\nUNION ALL \nSELECT 'SUNDAY', 0.9;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "source": "# Then, we can use the python name to turn cell2 into a Pandas dataframe\nmy_df = cell2.to_pandas()\n\n# Chart the data\nst.subheader(\"Chance of SNOW ‚ùÑÔ∏è\")\nst.line_chart(my_df, x='SNOWDAY', y='CHANCE_OF_SNOW')\n\n# Give it a go!\nst.subheader(\"Try it out yourself and show off your skills ü•á\")",
   "execution_count": null,
   "outputs": []
  }
 ]
}